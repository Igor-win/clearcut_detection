{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = '../data'\n",
    "IMAGES_PATH = 'images'\n",
    "MASKS_PATH = 'masks'\n",
    "INSTANCES_PATH = 'instance_masks'\n",
    "WIDHT , HEIGHT = 224, 224\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import imageio\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "def get_data_pathes(\n",
    "    datasets_path, images_path_name='images',\n",
    "    masks_path_name='masks', instances_path_name='instance_masks'\n",
    "    ):\n",
    "\n",
    "    datasets = list(os.walk(datasets_path))[0][1]\n",
    "    data_pathes = []\n",
    "    for dataset in datasets:\n",
    "        data_pathes.append((\n",
    "            os.path.join(datasets_path, dataset, images_path_name),\n",
    "            os.path.join(datasets_path, dataset, masks_path_name),\n",
    "            os.path.join(datasets_path, dataset, instances_path_name)))\n",
    "    \n",
    "    return data_pathes\n",
    "\n",
    "\n",
    "def get_instances(instances_path):\n",
    "    return list(os.walk(instances_path))[0][1]\n",
    "\n",
    "\n",
    "def image2mask(image_path, image_type):\n",
    "    return imageio.imread('{}.{}'.format(image_path, image_type))\n",
    "\n",
    "\n",
    "def get_data(\n",
    "    images_path, masks_path, instances,\n",
    "    img_type='tiff', msk_type='png'\n",
    "    ):\n",
    "\n",
    "    X = np.array([\n",
    "        image2mask(os.path.join(images_path, i), img_type) for i in instances])\n",
    "    y = np.array([\n",
    "        image2mask(os.path.join(masks_path, i), msk_type)for i in instances])\n",
    "    y = y.reshape([*y.shape, 1])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def get_area(instance_path):\n",
    "    return (gp.read_file(instance_path)['geometry'].area / 100).median()\n",
    "\n",
    "    \n",
    "def get_labels(distr):\n",
    "    res = np.full(distr.shape, 3)\n",
    "    res[distr < np.quantile(distr, 0.75)] = 2\n",
    "    res[distr < np.quantile(distr, 0.5)] = 1\n",
    "    res[distr < np.quantile(distr, 0.25)] = 0\n",
    "    return res\n",
    "\n",
    "\n",
    "def stratify(datasets_path, test_size, random_state):\n",
    "    images_path, masks_path, instances_path = get_data_pathes(datasets_path)[0]\n",
    "    instances = list(os.walk(instances_path))[0][1]\n",
    "\n",
    "    X, _ = get_data(images_path, masks_path, instances)\n",
    "    areas = np.array([\n",
    "        get_area(os.path.join(instances_path, i, i + '.geojson')) for i in instances])\n",
    "    labels = get_labels(areas)\n",
    "\n",
    "    sss = StratifiedShuffleSplit(\n",
    "        n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    return sss.split(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_info(dataset_path):\n",
    "    cols = [\n",
    "        'name', 'channel', 'position',\n",
    "        'image_path', 'mask_path', 'instance_path',\n",
    "        'image_type', 'mask_type'\n",
    "    ]\n",
    "    data_info = pd.DataFrame(columns=cols)\n",
    "    dataset = get_data_pathes(dataset_path)\n",
    "    for subset in dataset:\n",
    "        images_path, masks_path, instances_path = subset\n",
    "        instances = list(os.walk(instances_path))[0][1]\n",
    "        image_type = list(os.walk(images_path))[0][2][0].split('.')[-1]\n",
    "        mask_type = list(os.walk(masks_path))[0][2][0].split('.')[-1]\n",
    "\n",
    "        for i, instance in enumerate(instances):\n",
    "            instance = instance.split('_')\n",
    "            name = '_'.join(instance[:2])\n",
    "            channel = '_'.join(instance[2:-2])\n",
    "            position = '_'.join(instance[-2:])\n",
    "\n",
    "            data_info = data_info.append(\n",
    "                pd.DataFrame({\n",
    "                    'name': name,\n",
    "                    'channel': channel,\n",
    "                    'position': position,\n",
    "                    'image_path': images_path,\n",
    "                    'mask_path': masks_path,\n",
    "                    'instance_path': instances_path,\n",
    "                    'image_type': image_type,\n",
    "                    'mask_type': mask_type\n",
    "                }, index=[0]),\n",
    "                sort=True, ignore_index=True)\n",
    "    \n",
    "    return data_info\n",
    "\n",
    "\n",
    "def filter_by_channel(data_info, channel_name):\n",
    "    return data_info[data_info['channel'] == channel_name]\n",
    "\n",
    "\n",
    "def stratified_split(dataset_path, test_size=0.2, random_state=42):      \n",
    "    stratified_ix = stratify(dataset_path, test_size, random_state)\n",
    "    data_info = get_data_info(dataset_path)\n",
    "    filtered = filter_by_channel(\n",
    "        data_info,\n",
    "        data_info['channel'].values[0])\n",
    "    train_df = pd.DataFrame(columns=data_info.columns)\n",
    "    test_df = pd.DataFrame(columns=data_info.columns)\n",
    "    \n",
    "    for i, (train_ix, test_ix) in enumerate(stratified_ix):\n",
    "        for channel in data_info['channel'].unique():\n",
    "            row = data_info.iloc[train_ix]\n",
    "            row = row.replace(\n",
    "                row['channel'].values[0],\n",
    "                channel)\n",
    "            train_df = train_df.append(\n",
    "                row, sort=False,\n",
    "                ignore_index=True)\n",
    "            \n",
    "            row = data_info.iloc[test_ix]\n",
    "            row = row.replace(\n",
    "                row['channel'].values[0],\n",
    "                channel)\n",
    "            test_df = test_df.append(\n",
    "                row, sort=False,\n",
    "                ignore_index=True)\n",
    "    \n",
    "    return train_df, test_df  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = stratified_split(DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('../data/train_df.csv')\n",
    "test_df.to_csv('../data/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch_generator(\n",
    "    files_df, batch_size=4,\n",
    "    channels=['rgb', 'ndvi', 'ndvi_color', 'b2']\n",
    "    ):\n",
    "    \n",
    "    if len(channels) == 0:\n",
    "        raise Exception('You have to set at least 1 channel.')\n",
    "        \n",
    "    filtered = filter_by_channel(\n",
    "        files_df,\n",
    "        files_df['channel'].values[0])\n",
    "    while True:\n",
    "        for start in range(0, filtered.shape[0], batch_size):\n",
    "            images = []\n",
    "            masks = []\n",
    "            end = min(start + batch_size, filtered.shape[0])\n",
    "            train_batch = filtered.iloc[start:end]\n",
    "\n",
    "            for _, file in train_batch.iterrows():\n",
    "                res_image = []\n",
    "                res_mask = []\n",
    "                for channel in channels:\n",
    "                    row = files_df[\n",
    "                        (files_df['name']==file['name'])\n",
    "                        & (files_df['channel']==channel)\n",
    "                        & (files_df['position']==file['position'])\n",
    "                    ]\n",
    "                    filename = '_'.join([\n",
    "                        row['name'].values[0],\n",
    "                        channel,\n",
    "                        row['position'].values[0]])\n",
    "                    image_path = os.path.join(\n",
    "                        row['image_path'].values[0],\n",
    "                        '{}.{}'.format(filename, row['image_type'].values[0]))\n",
    "                    mask_path = os.path.join(\n",
    "                        row['mask_path'].values[0],\n",
    "                        '{}.{}'.format(filename, row['mask_type'].values[0]))\n",
    "                    print(channel, row['image_path'].values[0], sep='\\n')\n",
    "                    return\n",
    "                    img = imageio.imread(image_path)\n",
    "                    mask = imageio.imread(mask_path)\n",
    "\n",
    "                    res_image.append(img)\n",
    "                    res_mask.append(mask)\n",
    "                print(res_image)\n",
    "                return\n",
    "            \n",
    "            masks = np.array(masks, np.float32)\n",
    "            masks = masks.reshape(*masks.shape, 1)\n",
    "            \n",
    "            images = np.array(images, np.float32)\n",
    "            if images.ndim == 3:\n",
    "                images = images.reshape(*images.shape, 1)\n",
    "\n",
    "            yield images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb\n",
      "../data/20160103_66979721-be1b-4451-84e0-4a573236defd_ndvi/images\n"
     ]
    }
   ],
   "source": [
    "gen = build_batch_generator(train_df, 4)\n",
    "j = 0\n",
    "a = []\n",
    "for i in gen:\n",
    "    a.append(i)\n",
    "    print(i[0].shape)\n",
    "    print(i[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d0a00ac3121c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(a[1][0][0,:,:,2])\n",
    "plt.show()\n",
    "plt.imshow(a[1][1][0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = build_batch_generator(train_df, 4)\n",
    "val_generator = build_batch_generator(test_df, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "\n",
    "def freeze_model(model, freeze_before_layer):\n",
    "    if freeze_before_layer == \"ALL\":\n",
    "        for l in model.layers:\n",
    "            l.trainable = False\n",
    "    else:\n",
    "        freeze_before_layer_index = -1\n",
    "        for i, l in enumerate(model.layers):\n",
    "            if l.name == freeze_before_layer:\n",
    "                freeze_before_layer_index = i\n",
    "        for l in model.layers[:freeze_before_layer_index]:\n",
    "            l.trainable = False\n",
    "\n",
    "class ThreadsafeIter(object):\n",
    "    def __init__(self, it):\n",
    "        self.lock = threading.Lock()\n",
    "        self.it = it.__iter__()\n",
    "\n",
    "    def __iter__(self): return self\n",
    "\n",
    "    def __next__(self):\n",
    "        with self.lock:\n",
    "            return next(self.it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_models import Unet\n",
    "from segmentation_models.backbones import get_preprocessing\n",
    "from segmentation_models.losses import bce_dice_loss\n",
    "from segmentation_models.metrics import dice_score\n",
    "import math \n",
    "\n",
    "BACKBONE = 'resnet50'\n",
    "EPOCHS = 10\n",
    "BATCH = 4\n",
    "preprocess_input = get_preprocessing(BACKBONE)\n",
    "\n",
    "model = Unet(BACKBONE, encoder_weights='imagenet')\n",
    "model.compile('Adam', loss=bce_dice_loss, metrics=[dice_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "49/49 [==============================] - 229s 5s/step - loss: 1.0938 - score: 0.1394 - val_loss: 1.3525 - val_score: 0.0343\n",
      "Epoch 2/10\n",
      "49/49 [==============================] - 143s 3s/step - loss: 0.7832 - score: 0.3204 - val_loss: 0.9780 - val_score: 0.2583\n",
      "Epoch 3/10\n",
      "49/49 [==============================] - 140s 3s/step - loss: 0.7026 - score: 0.4009 - val_loss: 15.1306 - val_score: 0.0527\n",
      "Epoch 4/10\n",
      "49/49 [==============================] - 141s 3s/step - loss: 0.6884 - score: 0.4186 - val_loss: 3.1570 - val_score: 0.0719\n",
      "Epoch 5/10\n",
      "49/49 [==============================] - 140s 3s/step - loss: 0.6173 - score: 0.4824 - val_loss: 1.1790 - val_score: 0.1922\n",
      "Epoch 6/10\n",
      "49/49 [==============================] - 140s 3s/step - loss: 0.5801 - score: 0.5173 - val_loss: 8.9591 - val_score: 0.1226\n",
      "Epoch 7/10\n",
      "49/49 [==============================] - 139s 3s/step - loss: 0.5857 - score: 0.5152 - val_loss: 0.9020 - val_score: 0.3190\n",
      "Epoch 8/10\n",
      "49/49 [==============================] - 140s 3s/step - loss: 0.5853 - score: 0.5152 - val_loss: 1.0395 - val_score: 0.3023\n",
      "Epoch 9/10\n",
      "49/49 [==============================] - 140s 3s/step - loss: 0.5610 - score: 0.5372 - val_loss: 0.8776 - val_score: 0.3502\n",
      "Epoch 10/10\n",
      "49/49 [==============================] - 141s 3s/step - loss: 0.5468 - score: 0.5479 - val_loss: 0.8285 - val_score: 0.3229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fed25cb35c0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    ThreadsafeIter(train_generator),\n",
    "    steps_per_epoch=math.ceil(train_df.shape[0] / BATCH),\n",
    "    epochs = 10,\n",
    "    validation_data=ThreadsafeIter(val_generator),\n",
    "    validation_steps=math.ceil(test_df.shape[0] / BATCH),\n",
    "    max_queue_size=50,\n",
    "    workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
